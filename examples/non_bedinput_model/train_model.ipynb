{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "- train the example RBP model\n",
    "  - nothing fancy\n",
    "  - no concise dependencies \n",
    "  - just single-task for simplicity - PUM2\n",
    "- export the model to .json and weights to hdf5  \n",
    "\n",
    "\n",
    "## Structure\n",
    "\n",
    "Input files:\n",
    "\n",
    "- fasta\n",
    "- Bed\n",
    "- annotation GTF\n",
    "\n",
    "Input features:\n",
    "\n",
    "- 1-hot-encoded sequence\n",
    "- annotation features & distances\n",
    "\n",
    "I will start with the batch preprocessor (loading the whole dataset at once) and then add a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open questions\n",
    "\n",
    "- difference between genomelake and genomedatalayer?\n",
    "- I really like genomelake - it should become the standard for the pre-processors\n",
    "  - should be simple enough to use and understand\n",
    "  - add nearest feature-point extractor\n",
    "- test_files should be raw files not already pre-processed folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processor steps\n",
    "\n",
    "- Given range extract the sequence from the fasta file\n",
    "  - Validate the width (or just extract the center)\n",
    "  - compute the center range for it\n",
    "  - pybed tools\n",
    "    - [...] check the kundaje-lab code for this task\n",
    "\n",
    "- Load-in the GTF file and compute the distances to the nearest features\n",
    "  - [ ] see the programming possibilities for doing this in python\n",
    "  - see 4_append_other_positions.R\n",
    "  - just use pandas.DataFrame for it\n",
    "    - Idea: include it into concise as a pre-processor\n",
    "    \n",
    "- Why are they not using HTSeq?    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from concise.preprocessing import encodeDNA, EncodeSplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import concise.layers as cl\n",
    "import keras.layers as kl\n",
    "import concise.initializers as ci\n",
    "import concise.regularizers as cr\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONCISE_ROOT = \"/home/avsec/projects-work/concise/\"\n",
    "def load(split=\"train\", st=None):\n",
    "    dt = pd.read_csv(CONCISE_ROOT + \"/data/RBP/PUM2_{0}.csv\".format(split))\n",
    "    # DNA/RNA sequence\n",
    "    xseq = encodeDNA(dt.seq) \n",
    "    # distance to the poly-A site\n",
    "    xpolya = dt.polya_distance.as_matrix().reshape((-1, 1))\n",
    "    # response variable\n",
    "    y = dt.binding_site.as_matrix().reshape((-1, 1)).astype(\"float\")\n",
    "    return {\"seq\": xseq, \"dist_polya_raw\": xpolya}, y\n",
    "\n",
    "def data():\n",
    "    \n",
    "    train, valid, test = load(\"train\"), load(\"valid\"), load(\"test\")\n",
    "    \n",
    "    # transform the poly-A distance with B-splines\n",
    "    es = EncodeSplines()\n",
    "    es.fit(train[0][\"dist_polya_raw\"])\n",
    "    train[0][\"dist_polya_st\"] = es.transform(train[0][\"dist_polya_raw\"], warn=False)\n",
    "    valid[0][\"dist_polya_st\"] = es.transform(valid[0][\"dist_polya_raw\"], warn=False)\n",
    "    test[0][\"dist_polya_st\"] = es.transform(test[0][\"dist_polya_raw\"], warn=False)\n",
    "    \n",
    "    #return load(\"train\"), load(\"valid\"), load(\"test\")\n",
    "    return train + (es,), valid, test\n",
    "\n",
    "train, valid, test = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<concise.preprocessing.splines.EncodeSplines at 0x7f7a18cfad68>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - pass with the model (encodeSplines), trained pre-processor\n",
    "train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(train, filters=1, kernel_size=9, pwm_list=None, lr=0.001, ext_dist=False):\n",
    "    seq_length = train[0][\"seq\"].shape[1]\n",
    "    if pwm_list is None:\n",
    "        kinit = \"glorot_uniform\"\n",
    "        binit = \"zeros\"\n",
    "    else:\n",
    "        kinit = ci.PSSMKernelInitializer(pwm_list, add_noise_before_Pwm2Pssm=True)\n",
    "        binit = \"zeros\"\n",
    "        \n",
    "    # sequence\n",
    "    in_dna = cl.InputDNA(seq_length=seq_length, name=\"seq\")\n",
    "    inputs = [in_dna]\n",
    "    x = kl.Conv1D(filters=filters, \n",
    "                  kernel_size=kernel_size, \n",
    "                  activation=\"relu\",\n",
    "                  kernel_initializer=kinit,\n",
    "                  bias_initializer=binit,\n",
    "                  name=\"conv1\")(in_dna)\n",
    "    x = kl.AveragePooling1D(pool_size=4)(x)\n",
    "    x = kl.Flatten()(x)\n",
    "        \n",
    "    if ext_dist:    \n",
    "        # distance\n",
    "        in_dist = kl.Input(train[0][\"dist_polya_st\"].shape[1:], name=\"dist_polya_st\")\n",
    "        x_dist = cl.SplineT()(in_dist)\n",
    "        x = kl.concatenate([x, x_dist])\n",
    "        inputs += [in_dist]\n",
    "    \n",
    "    x = kl.Dense(units=1)(x)\n",
    "    m = Model(inputs, x)\n",
    "    m.compile(Adam(lr=lr), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model(train, filters=10, ext_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17713 samples, validate on 4881 samples\n",
      "Epoch 1/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3946 - acc: 0.8498 - val_loss: 0.4038 - val_acc: 0.8617\n",
      "Epoch 2/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3544 - acc: 0.8661 - val_loss: 0.4033 - val_acc: 0.8566\n",
      "Epoch 3/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3441 - acc: 0.8665 - val_loss: 0.3994 - val_acc: 0.8513\n",
      "Epoch 4/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3335 - acc: 0.8694 - val_loss: 0.4039 - val_acc: 0.8486\n",
      "Epoch 5/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3317 - acc: 0.8648 - val_loss: 0.4137 - val_acc: 0.8486\n",
      "Epoch 6/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3301 - acc: 0.8628 - val_loss: 0.3912 - val_acc: 0.8513\n",
      "Epoch 7/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3587 - acc: 0.8587 - val_loss: 0.4358 - val_acc: 0.8574\n",
      "Epoch 8/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3407 - acc: 0.8742 - val_loss: 0.3817 - val_acc: 0.8609\n",
      "Epoch 9/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3240 - acc: 0.8701 - val_loss: 0.4225 - val_acc: 0.8547\n",
      "Epoch 10/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3772 - acc: 0.8546 - val_loss: 0.4513 - val_acc: 0.8283\n",
      "Epoch 11/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3409 - acc: 0.8617 - val_loss: 0.3897 - val_acc: 0.8574\n",
      "Epoch 12/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3333 - acc: 0.8681 - val_loss: 0.4023 - val_acc: 0.8533\n",
      "Epoch 13/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3459 - acc: 0.8594 - val_loss: 0.3704 - val_acc: 0.8640\n",
      "Epoch 14/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3214 - acc: 0.8669 - val_loss: 0.4013 - val_acc: 0.8527\n",
      "Epoch 15/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3175 - acc: 0.8674 - val_loss: 0.4008 - val_acc: 0.8513\n",
      "Epoch 16/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3170 - acc: 0.8639 - val_loss: 0.3929 - val_acc: 0.8541\n",
      "Epoch 17/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3162 - acc: 0.8622 - val_loss: 0.3863 - val_acc: 0.8582\n",
      "Epoch 18/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3206 - acc: 0.8621 - val_loss: 0.6632 - val_acc: 0.7955\n",
      "Epoch 19/50\n",
      "17713/17713 [==============================] - 2s - loss: 0.3741 - acc: 0.8534 - val_loss: 0.4463 - val_acc: 0.8529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a0c163c88>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(train[0], train[1], epochs=50, validation_data=valid, \n",
    "      callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "seq (InputLayer)                 (None, 101, 4)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                   (None, 93, 10)        370         seq[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_6 (AveragePool (None, 23, 10)        0           conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dist_polya_st (InputLayer)       (None, 1, 10)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 230)           0           average_pooling1d_6[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "spline_t_6 (SplineT)             (None, 1)             10          dist_polya_st[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 231)           0           flatten_6[0][0]                  \n",
      "                                                                   spline_t_6[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 1)             232         concatenate_6[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 612\n",
      "Trainable params: 612\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.save_weights(\"model/weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_json = m.to_json()\n",
    "with open(\"model/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<concise.preprocessing.splines.EncodeSplines at 0x7f7a18cfad68>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"preprocessor/encodeSplines.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(train[2], output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_file = \"preprocessor/encodeSplines.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(es_file, \"rb\") as f:\n",
    "    es2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_model(arch=\"model/model.json\", weights=\"model/weights.h5\"):\n",
    "    from keras.models import model_from_json\n",
    "    with open(arch, 'r') as f:\n",
    "        m = model_from_json(f.read())\n",
    "    m.load_weights(\"model/weights.h5\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = read_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2006],\n",
       "       [ 0.4689],\n",
       "       [ 0.4009],\n",
       "       ..., \n",
       "       [-0.0013],\n",
       "       [ 0.1539],\n",
       "       [-0.0068]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the whole pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_model(arch=\"model/model.json\", weights=\"model/weights.h5\"):\n",
    "    from keras.models import model_from_json\n",
    "    with open(arch, 'r') as f:\n",
    "        m = model_from_json(f.read())\n",
    "    m.load_weights(\"model/weights.h5\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-processor\n",
    "from concise.utils.helper import read_json\n",
    "from preprocessor import preprocessor\n",
    "# kwargs\n",
    "pp_kwargs = read_json(\"preprocessor_test_kwargs.json\")\n",
    "pp = preprocessor(**pp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model\n",
    "m = read_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unpack\n",
    "pp_inputs = map(lambda x: x[\"inputs\"], pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2017-08-11 12:01:02,771:genomelake] Running landmark extractors..\n",
      "2017-08-11 12:01:02,771 [INFO] Running landmark extractors..\n",
      "INFO:2017-08-11 12:01:02,785:genomelake] Done!\n",
      "2017-08-11 12:01:02,785 [INFO] Done!\n"
     ]
    }
   ],
   "source": [
    "res = np.concatenate([m.predict_on_batch(next(pp_inputs)) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2593],\n",
       "       [ 0.2135],\n",
       "       [ 0.2135],\n",
       "       [ 0.2593],\n",
       "       [ 0.2593],\n",
       "       [ 0.2593],\n",
       "       [ 0.2135],\n",
       "       [ 0.2593],\n",
       "       [ 0.2593],\n",
       "       [ 0.2593],\n",
       "       [ 0.2593],\n",
       "       [ 0.2593]], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - troubled\n",
    "#res = m.predict_generator(pp_inputs, steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "- pack everything into the pipeline"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
