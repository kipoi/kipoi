{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributing a model to the Kipoi model repository\n",
    "\n",
    "This notebook will show you how to contribute a model to the [Kipoi model repository](https://github.com/kipoi/models). For a simple 'model contribution checklist' see also <http://kipoi.org/docs/contributing/01_Getting_started/>.\n",
    "\n",
    "## Kipoi basics\n",
    "\n",
    "Contributing a model to Kipoi means writing a sub-folder with all the required files to the [Kipoi model repository](https://github.com/kipoi/models) via pull request.\n",
    "\n",
    "Two main components of the model repository are **model** and **dataloader**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](../docs/theme_dir/img/kipoi-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Model takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the `predict_on_batch(x)` method, where `x` is dictionary/list of numpy arrays. The model contributor needs to provide one of the following:\n",
    "\n",
    "- Serialized Keras model\n",
    "- Serialized Sklearn model\n",
    "- Custom model inheriting from `keras.model.BaseModel`.\n",
    "  - all the required files, i.e. weights need to be loaded in the `__init__`\n",
    "  \n",
    "See <http://kipoi.org/docs/contributing/02_Writing_model.yaml/> and <http://kipoi.org/docs/contributing/05_Writing_model.py/> for more info.\n",
    "\n",
    "### Dataloader\n",
    "\n",
    "Dataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. \n",
    "\n",
    "Before writing your own dataloader take a look at our [kipoiseq](https://github.com/kipoi/kipoiseq) repository to see whether your use-case is covered by the available dataloaders.\n",
    "\n",
    "#### Writing your own dataloader\n",
    "Technically, dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following:\n",
    "\n",
    "- PreloadedDataset\n",
    "- Dataset\n",
    "- BatchDataset\n",
    "- SampleIterator\n",
    "- BatchIterator\n",
    "- SampleGenerator\n",
    "- BatchGenerator\n",
    "\n",
    "See <http://kipoi.org/docs/contributing/04_Writing_dataloader.py/> for more info.\n",
    "\n",
    "### Folder layout\n",
    "\n",
    "Here is an example folder structure of a Kipoi model:\n",
    "\n",
    "```\n",
    "MyModel\n",
    "├── dataloader.py     # implements the dataloader (only necessary if you wrote your own dataloader)\n",
    "├── dataloader.yaml   # describes the dataloader (only necessary if you wrote your own dataloader)\n",
    "└── model.yaml         # describes the model\n",
    "```    \n",
    "\n",
    "The `model.yaml` and `dataloader.yaml` files a complete description about the model, the dataloader and the files they depend on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing a simple Iris-classifier\n",
    "\n",
    "Details about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known [Iris](archive.ics.uci.edu/ml/datasets/Iris) dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "1. Train the model\n",
    "2. Generate `dataloader_files/`\n",
    "3. Generate `model_files/`\n",
    "4. Generate `example_files/`\n",
    "5. Write `model.yaml`\n",
    "6. Write `dataloader.yaml`\n",
    "7. Write `dataloader.py`\n",
    "8. Test with the model with `$ kipoi test .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view more info about the dataset\n",
    "# print(iris[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "y_transformer = LabelBinarizer().fit(iris[\"target\"])\n",
    "x_transformer = StandardScaler().fit(iris[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_transformer.transform(iris[\"data\"])\n",
    "y = y_transformer.transform(iris[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90068117,  1.03205722, -1.3412724 , -1.31297673],\n",
       "       [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673],\n",
       "       [-1.38535265,  0.33784833, -1.39813811, -1.31297673]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train an example model\n",
    "\n",
    "Let's train a simple linear-regression model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b28f8c04a58>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.layers as kl\n",
    "\n",
    "inp = kl.Input(shape=(4, ), name=\"features\")\n",
    "out = kl.Dense(units=3)(inp)\n",
    "model = Model(inp, out)\n",
    "model.compile(\"adam\", \"categorical_crossentropy\")\n",
    "\n",
    "model.fit(x, y, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set the model directory up:\n",
    "\n",
    "In reality, you would also need to \n",
    "\n",
    "1. Fork the [kipoi/models repository](https://github.com/kipoi/models)\n",
    "2. Clone your repository fork, ignoring all the git-lfs files\n",
    "    - `$ git clone git@github.com:<your_username>/models.git`\n",
    "3. Create a new folder `<mynewmodel>` and start setting up the `model.yaml` (and `dataloader.yaml` and `dataloader.py` files if necessary)\n",
    "\n",
    "\n",
    "### 3. Store the files in a temporary directory\n",
    "All the data of the model will have to be published on zenodo or figshare before the pull request is performed. While setting the Kipoi model up, it is handy the keep the models in a temporary directory in the model folder, which we will delete prior to the pull request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘contribution_sample_model’: File exists\n",
      "mkdir: cannot create directory ‘contribution_sample_model/tmp’: File exists\n"
     ]
    }
   ],
   "source": [
    "# create the model directory\n",
    "!mkdir contribution_sample_model\n",
    "# create the temporary directory where we will keep the files that should later be published in zenodo or figshare\n",
    "!mkdir contribution_sample_model/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can change the current working directory to the model directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"contribution_sample_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Static files for dataloader\n",
    "Since in our case here we require to write a new dataloader. The dataloader can use some trained transformer instances (here the `LabelBinarizer` and `StandardScaler` transformers form sklearn). These should be uploaded with the model files and then referenced correctly in the `dataloader.yaml` file. We will store the required files in the temporary folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp/y_transformer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y_transformer, f, protocol=2)\n",
    "\n",
    "with open(\"tmp/x_transformer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(x_transformer, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_transformer.pkl  y_transformer.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! ls tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Model definition / weights\n",
    "Now that we have the static files that are required by the dataloader, we also need to store the model architecture and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "with open(\"tmp/model.json\", \"w\") as f:\n",
    "    f.write(model.to_json())\n",
    "    \n",
    "# Weights\n",
    "model.save_weights(\"tmp/weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively if we would be using a scikit-learn model we would save the pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, for the scikit-learn model we would save the pickle file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "lr = OneVsRestClassifier(LogisticRegression())\n",
    "lr.fit(x, y)\n",
    "\n",
    "with open(\"tmp/sklearn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr, f, protocol=2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Example files for the dataloader\n",
    "Every Kipoi dataloader has to provide a set of example files so that Kipoi can perform its automated tests and users can have an idea what the dataloader files have to look like. Again we will store the files in the temporary folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first 20 rows of the iris dataset\n",
    "X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"])\n",
    "y = pd.DataFrame({\"class\": iris[\"target\"][:20]})\n",
    "# store the model input features and targets as csv files with column names:\n",
    "X.to_csv(\"tmp/example_features.csv\", index=False)\n",
    "y.to_csv(\"tmp/example_targets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d. Write the model.yaml\n",
    "Now it is time to write the model.yaml in the model directory. Since we are in the testing stage we will be using local file paths in the `args` field - those will be replaced by zenodo links once everything is ready for publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yaml = \"\"\"\n",
    "type: kipoi.model.KerasModel  # use `kipoi.model.KerasModel`\n",
    "args:  # arguments of `kipoi.model.KerasModel`\n",
    "    arch: tmp/model.json\n",
    "    weights: tmp/weights.h5\n",
    "default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory\n",
    "info: # General information about the model\n",
    "    authors: \n",
    "        - name: Your Name\n",
    "          github: your_github_username\n",
    "          email: your_email@host.org\n",
    "    doc: Model predicting the Iris species\n",
    "    version: 0.1  # optional \n",
    "    cite_as: https://doi.org:/... # preferably a doi url to the paper\n",
    "    trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description\n",
    "    license: MIT # Software License - defaults to MIT\n",
    "dependencies:\n",
    "    conda: # install via conda\n",
    "      - python=3.5\n",
    "      - h5py\n",
    "      # - soumith::pytorch  # specify packages from other channels via <channel>::<package>      \n",
    "    pip:   # install via pip\n",
    "      - keras>=2.0.4\n",
    "      - tensorflow>=1.0\n",
    "schema:  # Model schema\n",
    "    inputs:\n",
    "        features:\n",
    "            shape: (4,)  # array shape of a single sample (omitting the batch dimension)\n",
    "            doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\"\n",
    "    targets:\n",
    "        shape: (3,)\n",
    "        doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\n",
    "\"\"\"\n",
    "with open(\"model.yaml\", \"w\") as ofh:\n",
    "    ofh.write(model_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3e. Write the dataloader.yaml\n",
    "Now it is time to write the `dataloader.yaml`. Since we defined the `default_dataloader` field in `model.yaml` as `.` Kipoi will expect that our `dataloader.yaml` file lies in the same directory. Since we are in the testing stage we will be using local file paths in the `args` field - those will be replaced by zenodo links once everything is ready for publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_yaml = \"\"\"\n",
    "type: Dataset\n",
    "defined_as: dataloader.MyDataset\n",
    "args:\n",
    "    features_file:\n",
    "        # descr: > allows multi-line fields\n",
    "        doc: >\n",
    "          Csv file of the Iris Plants Database from\n",
    "          http://archive.ics.uci.edu/ml/datasets/Iris features.\n",
    "        type: str\n",
    "        example: tmp/features.csv  # example files\n",
    "    targets_file:\n",
    "        doc: >\n",
    "          Csv file of the Iris Plants Database targets.\n",
    "          Not required for making the prediction.\n",
    "        type: str\n",
    "        example: tmp/targets.csv\n",
    "        optional: True  # if not present, the `targets` field will not be present in the dataloader output\n",
    "    x_transformer:\n",
    "        default: tmp/x_transformer.pkl\n",
    "    y_transformer:\n",
    "        default: tmp/y_transformer.pkl\n",
    "    \n",
    "info:\n",
    "    authors: \n",
    "        - name: Your Name\n",
    "          github: your_github_account\n",
    "          email: your_email@host.org\n",
    "    version: 0.1\n",
    "    doc: Model predicting the Iris species\n",
    "dependencies:\n",
    "    conda:\n",
    "      - python=3.5\n",
    "      - pandas\n",
    "      - numpy\n",
    "      - sklearn\n",
    "output_schema:\n",
    "    inputs:\n",
    "        features:\n",
    "            shape: (4,)\n",
    "            doc: Features in cm: sepal length, sepal width, petal length, petal width.\n",
    "    targets:\n",
    "        shape: (3, )\n",
    "        doc: One-hot encoded array of classes: setosa, versicolor, virginica.\n",
    "    metadata:  # field providing additional information to the samples (not directly required by the model)\n",
    "        example_row_number:\n",
    "            shape: int\n",
    "            doc: Just an example metadata column\n",
    "\"\"\"\n",
    "with open(\"dataloader.yaml\", \"w\") as ofh:\n",
    "    ofh.write(dataloader_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have referred to the dataloader as `dataloader.MyDataset` we expect a `dataloader.py` file in the same directory as `dataloader.yaml` which has to contain the dataloader class, which is here `MyDataset`.\n",
    "\n",
    "Notice that external static files are arguments to the `__init__` function! Their path was defined in the `dataloader.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from kipoi.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_pickle(f):\n",
    "    with open(f, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features_file, x_transformer, y_transformer, targets_file=None):\n",
    "        self.features_file = features_file\n",
    "        self.targets_file = targets_file\n",
    "\n",
    "        self.y_transformer = read_pickle(x_transformer)\n",
    "        self.x_transformer = read_pickle(y_transformer)\n",
    "\n",
    "        self.features = pd.read_csv(features_file)\n",
    "        if targets_file is not None:\n",
    "            self.targets = pd.read_csv(targets_file)\n",
    "            assert len(self.targets) == len(self.features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis]))\n",
    "        if self.targets_file is None:\n",
    "            y_class = {}\n",
    "        else:\n",
    "            y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis]))\n",
    "        return {\n",
    "            \"inputs\": {\n",
    "                \"features\": x_features\n",
    "            },\n",
    "            \"targets\": y_class,\n",
    "            \"metadata\": {\n",
    "                \"example_row_number\": idx\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate `dataloader_files/`\n",
    "\n",
    "Now that we have everything we need, let's start writing the files to model's directory (here `model_template/`). \n",
    "\n",
    "In reality, you would need to \n",
    "\n",
    "1. Fork the [kipoi/models repository](https://github.com/kipoi/models)\n",
    "2. Clone your repository fork, ignoring all the git-lfs files\n",
    "    - `$ git clone git@github.com:<your_username>/models.git '-I /'`\n",
    "3. Publish your model weights on figshare or zenodo. Make sure you have the download URL and the md5sum ready (both values are available on the website for all files individually).\n",
    "3. Create a new folder `<mynewmodel>` containing all the `model.yaml` (and `dataloader.yaml` and `dataloader.py` files if necessary) in the repostiory root\n",
    "4. Test your repository locally:\n",
    "    - `$ kipoi test <mynewmodel_folder>`\n",
    "5. Commit, push to your forked remote and submit a pull request to [github.com/kipoi/models](https://github.com/kipoi/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case here the we require to write a new dataloader. The dataloader can use some trained transformer instances (here the `LabelBinarizer` and `StandardScaler` transformers form sklearn). These should be uploaded with the model files and then referenced correctly in the `dataloader.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/nasif12/home_if12/avsec/workspace/kipoi/kipoi/examples/sklearn_iris\n"
     ]
    }
   ],
   "source": [
    "cd ../examples/sklearn_iris/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM HERE ONWARDS UPDATE ONCE EXTRA FILE URLS ARE ALLOWED IN YAML FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"dataloader_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;27mdataloader_files\u001b[0m/  dataloader.pyc   \u001b[38;5;27mexample_files\u001b[0m/  model.yaml\r\n",
      "dataloader.py      dataloader.yaml  \u001b[38;5;27mmodel_files\u001b[0m/    \u001b[38;5;27m__pycache__\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataloader_files/y_transformer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y_transformer, f, protocol=2)\n",
    "\n",
    "with open(\"dataloader_files/x_transformer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(x_transformer, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_transformer.pkl  y_transformer.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls dataloader_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate `model_files/`\n",
    "\n",
    "The serialized model weights and architecture go to `model_files/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"model_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "with open(\"model_files/model.json\", \"w\") as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "model.save_weights(\"model_files/weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, for the scikit-learn model we would save the pickle file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "lr = OneVsRestClassifier(LogisticRegression())\n",
    "lr.fit(x, y)\n",
    "\n",
    "with open(\"model_files/sklearn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr, f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate `example_files/`\n",
    "\n",
    "`example_files/` should contain a small subset of the raw files the dataloader will read.\n",
    "\n",
    "#### Numpy arrays -> pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target_names', 'feature_names', 'data', 'DESCR', 'target'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame({\"class\": iris[\"target\"][:20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save example files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"example_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"example_files/features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv(\"example_files/targets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 example_files/targets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm),sepal width (cm),petal length (cm),petal width (cm)\n",
      "5.1,3.5,1.4,0.2\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 example_files/features.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write `model.yaml`\n",
    "\n",
    "The `model.yaml` for this model should look like this:\n",
    "\n",
    "```yaml\n",
    "type: keras  # use `kipoi.model.KerasModel`\n",
    "args:  # arguments of `kipoi.model.KerasModel`\n",
    "    arch: model_files/model.json\n",
    "    weights: model_files/weights.h5\n",
    "default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory\n",
    "info: # General information about the model\n",
    "    authors: \n",
    "        - name: Your Name\n",
    "          github: your_github_username\n",
    "          email: your_email@host.org\n",
    "    doc: Model predicting the Iris species\n",
    "    version: 0.1  # optional \n",
    "    cite_as: https://doi.org:/... # preferably a doi url to the paper\n",
    "    trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description\n",
    "    license: MIT # Software License - defaults to MIT\n",
    "dependencies:\n",
    "    conda: # install via conda\n",
    "      - python=3.5\n",
    "      - h5py\n",
    "      # - soumith::pytorch  # specify packages from other channels via <channel>::<package>      \n",
    "    pip:   # install via pip\n",
    "      - keras>=2.0.4\n",
    "      - tensorflow>=1.0\n",
    "schema:  # Model schema\n",
    "    inputs:\n",
    "        features:\n",
    "            shape: (4,)  # array shape of a single sample (omitting the batch dimension)\n",
    "            doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\"\n",
    "    targets:\n",
    "        shape: (3,)\n",
    "        doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\"\n",
    "```\n",
    "\n",
    "All file paths are relative relative to `model.yaml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Write `dataloader.yaml`\n",
    "\n",
    "```yaml\n",
    "type: Dataset\n",
    "defined_as: dataloader.py::MyDataset  # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py\n",
    "args:\n",
    "    features_file:\n",
    "        # descr: > allows multi-line fields\n",
    "        doc: >\n",
    "          Csv file of the Iris Plants Database from\n",
    "          http://archive.ics.uci.edu/ml/datasets/Iris features.\n",
    "        type: str\n",
    "        example: example_files/features.csv  # example files\n",
    "    targets_file:\n",
    "        doc: >\n",
    "          Csv file of the Iris Plants Database targets.\n",
    "          Not required for making the prediction.\n",
    "        type: str\n",
    "        example: example_files/targets.csv\n",
    "        optional: True  # if not present, the `targets` field will not be present in the dataloader output\n",
    "info:\n",
    "    authors: \n",
    "        - name: Your Name\n",
    "          github: your_github_account\n",
    "          email: your_email@host.org\n",
    "    version: 0.1\n",
    "    doc: Model predicting the Iris species\n",
    "dependencies:\n",
    "    conda:\n",
    "      - python=3.5\n",
    "      - pandas\n",
    "      - numpy\n",
    "      - sklearn\n",
    "output_schema:\n",
    "    inputs:\n",
    "        features:\n",
    "            shape: (4,)\n",
    "            doc: Features in cm: sepal length, sepal width, petal length, petal width.\n",
    "    targets:\n",
    "        shape: (3, )\n",
    "        doc: One-hot encoded array of classes: setosa, versicolor, virginica.\n",
    "    metadata:  # field providing additional information to the samples (not directly required by the model)\n",
    "        example_row_number:\n",
    "            shape: int\n",
    "            doc: Just an example metadata column\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write `dataloader.py`\n",
    "\n",
    "Finally, let's implement MyDataset. We need to implement two methods: `__len__` and `__getitem__`. \n",
    "\n",
    "`__getitem__` will return one item of the dataset. In our case, this is a dictionary with `output_schema` described in `dataloader.yaml`.\n",
    "\n",
    "For more information about writing such dataloaders, see the [Data Loading and Processing Tutorial from pytorch](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from kipoi.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_pickle(f):\n",
    "    with open(f, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features_file, targets_file=None):\n",
    "        self.features_file = features_file\n",
    "        self.targets_file = targets_file\n",
    "\n",
    "        self.y_transformer = read_pickle(\"dataloader_files/y_transformer.pkl\")\n",
    "        self.x_transformer = read_pickle(\"dataloader_files/x_transformer.pkl\")\n",
    "\n",
    "        self.features = pd.read_csv(features_file)\n",
    "        if targets_file is not None:\n",
    "            self.targets = pd.read_csv(targets_file)\n",
    "            assert len(self.targets) == len(self.features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis]))\n",
    "        if self.targets_file is None:\n",
    "            y_class = {}\n",
    "        else:\n",
    "            y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis]))\n",
    "        return {\n",
    "            \"inputs\": {\n",
    "                \"features\": x_features\n",
    "            },\n",
    "            \"targets\": y_class,\n",
    "            \"metadata\": {\n",
    "                \"example_row_number\": idx\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MyDataset(\"example_files/features.csv\", \"example_files/targets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': {'features': array([-0.5372,  1.9577, -1.1707, -1.05  ])},\n",
       " 'metadata': {'example_row_number': 5},\n",
       " 'targets': array([1, 0, 0])}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call __getitem__\n",
    "ds[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since MyDatset inherits from `kipoi.data.Dataset`, it has some additional nice feature. See [python-sdk.ipynb](python-sdk.ipynb) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': {'features': array([[-0.9007,  1.0321, -1.3413, -1.313 ],\n",
       "         [-1.143 , -0.125 , -1.3413, -1.313 ],\n",
       "         [-1.3854,  0.3378, -1.3981, -1.313 ]])},\n",
       " 'metadata': {'example_row_number': array([0, 1, 2])},\n",
       " 'targets': array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch-iterator\n",
    "it = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.load_all()  # load the whole dataset into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test with the model with `$ kipoi test .`\n",
    "\n",
    "Before we contribute the model to the repository, let's run the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from dataloader.py::MyDataset\u001b[0m\n",
      "Using TensorFlow backend.\n",
      "2017-11-29 17:26:21.755321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-11-29 17:26:21.755368: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-11-29 17:26:21.755385: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-11-29 17:26:21.755399: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-11-29 17:26:21.755414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='model_files/model.json' mode='r' encoding='UTF-8'>\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from model_files/weights.h5\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m\n",
      "/opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 89.45it/s]\n",
      "\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Successfully ran test_predict\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!kipoi test ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command did the following:\n",
    "\n",
    "- validated if `output_schema` defined in `dataloader.yaml` matches the shapes of the returned arrays\n",
    "- validated that model and dataloader are compatible in `inputs` and `targets`\n",
    "- executed the model pipeline for the example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the model through kipoi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kipoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'kipoi' from '/data/nasif12/home_if12/avsec/projects-work/kipoi/kipoi/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(kipoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "m = kipoi.get_model(\".\", source=\"dir\")  # See also python-sdk.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5356, -0.8118, -0.2712],\n",
       "       [ 0.4649, -0.22  , -1.1491],\n",
       "       [ 0.6735, -0.1923, -0.8083],\n",
       "       [ 0.3958,  0.0178, -0.9159],\n",
       "       [ 1.6362, -0.79  , -0.0849]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pipeline.predict({\"features_file\": \"example_files/features.csv\", \"targets_file\": \"example_files/targets.csv\" })[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Info(authors=[Author(name='Your Name', github='your_github_username', email=None)], doc='Model predicting the Iris species', name=None, version='0.1', tags=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataloader.MyDataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.default_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f0dbe68f8d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f0dc19cf400>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict_on_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "Congrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use `kipoi init` to setup a model directory. Make sure you have read the [getting started guide](http://kipoi.org/docs/contributing/01_Getting_started/) for contributing models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
