from __future__ import absolute_import
from __future__ import print_function

import os
import logging
import abc

import kipoi  # for .config module
from kipoi.components import DataLoaderDescription
from .utils import load_module, cd, getargs
from .external.torch.data import DataLoader
from kipoi.data_utils import numpy_collate, numpy_collate_concat, get_dataset_item, get_dataset_lens
from tqdm import tqdm


_logger = logging.getLogger('kipoi')

#
PREPROC_IFILE_TYPES = ['DNA_regions']
PREPROC_IFILE_FORMATS = ['bed3']


class BaseDataLoader(object):
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def batch_iter(self, **kwargs):
        raise NotImplementedError

    def batch_train_iter(self, **kwargs):
        """Returns samples directly useful for training the model:
        (x["inputs"],x["targets"])
        """
        return ((x["inputs"], x["targets"]) for x in self.batch_iter(**kwargs))

    def batch_predict_iter(self, **kwargs):
        """Returns samples directly useful for prediction x["inputs"]
        """
        return (x["inputs"] for x in self.batch_iter(**kwargs))

    @abc.abstractmethod
    def load_all(self):
        raise NotImplementedError

# --------------------------------------------
# Different implementations

# Other options:
# - generator - sample, batch-based
#   - yield.
# - iterator, iterable - sample, batch-based
#   - __iter__()
#     - __next__()
# - full dataset
#   - everything numpy arrays with the same first axis length


class PreloadedDataset(BaseDataLoader):
    """Generated by supplying a function returning the full dataset.

    The full dataset is a nested (list/dict) python structure of numpy arrays
    with the same first axis dimension.
    """
    data_fn = None

    @classmethod
    def from_data_fn(cls, data_fn):
        """setup the class variable
        """
        cls.data_fn = data_fn
        return cls

    @classmethod
    def from_data(cls, data):
        return cls.from_data_fn(lambda: data)()

    @classmethod
    def get_data_fn(cls):
        assert cls.data_fn is not None
        return cls.data_fn

    def __init__(self, *args, **kwargs):
        self.data = self.get_data_fn()(*args, **kwargs)
        lens = get_dataset_lens(self.data, require_numpy=True)
        # check that all dimensions are the same
        assert len(set(lens)) == 1
        self.n = lens[0]

    def __len__(self):
        return self.n

    def __getitem__(self, index):
        return get_dataset_item(self.data, index)

    def batch_iter(self, batch_size=32, shuffle=False, drop_last=False, **kwargs):
        """Return a batch-iterator

        Arguments:
            dataset (Dataset): dataset from which to load the data.
            batch_size (int, optional): how many samples per batch to load
                (default: 1).
            shuffle (bool, optional): set to ``True`` to have the data reshuffled
                at every epoch (default: False).
            num_workers (int, optional): how many subprocesses to use for data
                loading. 0 means that the data will be loaded in the main process
                (default: 0)
            drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,
                if the dataset size is not divisible by the batch size. If False and
                the size of dataset is not divisible by the batch size, then the last batch
                will be smaller. (default: False)

        Returns:
            iterator
        """
        dl = DataLoader(self, batch_size=batch_size,
                        collate_fn=numpy_collate,
                        shuffle=shuffle,
                        num_workers=0,
                        drop_last=drop_last)
        return iter(dl)

    def load_all(self, **kwargs):
        """Load the whole dataset into memory
        Arguments:
            batch_size (int, optional): how many samples per batch to load
                (default: 1).
            num_workers (int, optional): how many subprocesses to use for data
                loading. 0 means that the data will be loaded in the main process
                (default: 0)
        """
        return self.data


class Dataset(BaseDataLoader):
    """An abstract class representing a Dataset.

    All other datasets should subclass it. All subclasses should override
    ``__len__``, that provides the size of the dataset, and ``__getitem__``,
    supporting integer indexing in range from 0 to len(self) exclusive.
    """

    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def __getitem__(self, index):
        """Return one sample

        index: {0, ..., len(self)-1}
        """
        raise NotImplementedError

    @abc.abstractmethod
    def __len__(self):
        """Return the number of all samples
        """
        raise NotImplementedError

    def batch_iter(self, batch_size=32, shuffle=False, num_workers=0, drop_last=False, **kwargs):
        """Return a batch-iterator

        Arguments:
            dataset (Dataset): dataset from which to load the data.
            batch_size (int, optional): how many samples per batch to load
                (default: 1).
            shuffle (bool, optional): set to ``True`` to have the data reshuffled
                at every epoch (default: False).
            num_workers (int, optional): how many subprocesses to use for data
                loading. 0 means that the data will be loaded in the main process
                (default: 0)
            drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,
                if the dataset size is not divisible by the batch size. If False and
                the size of dataset is not divisible by the batch size, then the last batch
                will be smaller. (default: False)

        Returns:
            iterator
        """
        dl = DataLoader(self, batch_size=batch_size,
                        collate_fn=numpy_collate,
                        shuffle=shuffle,
                        num_workers=num_workers,
                        drop_last=drop_last)
        return iter(dl)

    def load_all(self, batch_size=32, num_workers=0, **kwargs):
        """Load the whole dataset into memory
        Arguments:
            batch_size (int, optional): how many samples per batch to load
                (default: 1).
            num_workers (int, optional): how many subprocesses to use for data
                loading. 0 means that the data will be loaded in the main process
                (default: 0)
        """
        return numpy_collate_concat([x for x in tqdm(self.batch_iter(batch_size,
                                                                     num_workers=num_workers))])


class BatchDataset(BaseDataLoader):
    """An abstract class representing a BatchDataset.
    """

    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def __getitem__(self, index):
        """Return one batch
        """
        raise NotImplementedError

    @abc.abstractmethod
    def __len__(self):
        """Number of all batches
        """
        raise NotImplementedError

    def batch_iter(self, num_workers=0, **kwargs):
        """Return a batch-iterator

        Arguments:
            dataset (Dataset): dataset from which to load the data.
            batch_size (int, optional): how many samples per batch to load
                (default: 1).
            shuffle (bool, optional): set to ``True`` to have the data reshuffled
                at every epoch (default: False).
            num_workers (int, optional): how many subprocesses to use for data
                loading. 0 means that the data will be loaded in the main process
                (default: 0)
            drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,
                if the dataset size is not divisible by the batch size. If False and
                the size of dataset is not divisible by the batch size, then the last batch
                will be smaller. (default: False)

        Returns:
            iterator
        """
        dl = DataLoader(self, batch_size=1,
                        collate_fn=numpy_collate_concat,
                        shuffle=False,
                        num_workers=num_workers,
                        drop_last=False)
        return iter(dl)

    def load_all(self, num_workers=0, **kwargs):
        """Load the whole dataset into memory
        Arguments:
            num_workers (int, optional): how many subprocesses to use for data
                loading. 0 means that the data will be loaded in the main process
                (default: 0)
        """
        return numpy_collate_concat([x for x in tqdm(self.batch_iter(num_workers=num_workers))])


class SampleIterator(BaseDataLoader):
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def __iter__(self):
        raise NotImplementedError

    # TODO - how to maintain compatibility with python2?
    @abc.abstractmethod
    def __next__(self):
        raise NotImplementedError

    next = __next__

    def batch_iter(self, batch_size=32, **kwargs):
        # TODO - implement this in parallel - add `num_workers` argument
        # https://github.com/fchollet/keras/blob/master/keras/utils/data_utils.py#L589
        l = []
        for x in iter(self):
            l.append(x)
            if len(l) == batch_size:
                ret = numpy_collate(l)
                # remove all elements
                del l[:]
                yield ret
        # Return the rest
        if len(l) > 0:
            yield numpy_collate(l)

    def load_all(self, batch_size=32, num_workers=0, **kwargs):
        return numpy_collate_concat([x for x in tqdm(self.batch_iter(batch_size,
                                                                     num_workers=num_workers))])


class BatchIterator(BaseDataLoader):
    @abc.abstractmethod
    def __iter__(self):
        raise NotImplementedError

    # TODO - how to maintain compatibility with python2?
    @abc.abstractmethod
    def __next__(self):
        raise NotImplementedError

    next = __next__

    def batch_iter(self, **kwargs):
        # TODO - implement num_workers
        return iter(self)

    def load_all(self, num_workers=0, **kwargs):
        return numpy_collate_concat([x for x in tqdm(self.batch_iter(num_workers=num_workers))])


# Transform a generator into a class

class SampleGenerator(SampleIterator):
    """Transform a generator of samples into SampleIterator
    """
    generator_fn = None

    @classmethod
    def from_generator_fn(cls, generator_fn):
        """setup the class variable
        """
        cls.generator_fn = generator_fn
        return cls

    @classmethod
    def get_generator_fn(cls):
        assert cls.generator_fn is not None
        return cls.generator_fn

    def __init__(self, *args, **kwargs):
        self.gen = self.get_generator_fn()(*args, **kwargs)

    def __iter__(self):
        return self

    def __next__(self):
        return self.gen.__next__()


class BatchGenerator(BatchIterator):
    """Transform a generator of batches into BatchIterator
    """
    generator_fn = None

    @classmethod
    def from_generator_fn(cls, generator_fn):
        cls.generator_fn = generator_fn
        return cls

    @classmethod
    def get_generator_fn(cls):
        assert cls.generator_fn is not None
        return cls.generator_fn

    def __init__(self, *args, **kwargs):
        self.gen = self.get_generator_fn()(*args, **kwargs)

    def __iter__(self):
        return self

    def __next__(self):
        return self.gen.__next__()


# --------------------------------------------

def get_dataloader_factory(dataloader, source="kipoi"):

    # pull the dataloader & get the dataloader directory
    source = kipoi.config.get_source(source)
    yaml_path = source.pull_dataloader(dataloader)
    dataloader_dir = os.path.dirname(yaml_path)

    # Setup dataloader description
    dl = DataLoaderDescription.load(yaml_path)
    # --------------------------------------------
    # input the
    file_path, obj_name = tuple(dl.defined_as.split("::"))
    CustomDataLoader = getattr(load_module(os.path.join(dataloader_dir, file_path)),
                               obj_name)

    # check that dl.type is correct
    if dl.type not in AVAILABLE_DATALOADERS:
        raise ValueError("dataloader type: {0} is not in supported dataloaders:{1}".
                         format(dl.type, list(AVAILABLE_DATALOADERS.keys())))
    # check that CustomDataLoader indeed interits from the right DataLoader
    if not issubclass(CustomDataLoader, AVAILABLE_DATALOADERS[dl.type]):
        raise ValueError("DataLoader does't inherit from the specified dataloader: {0}".
                         format(AVAILABLE_DATALOADERS[dl.type].__name__))
    # check that the extractor arguments match yml arguments
    if not getargs(CustomDataLoader) == set(dl.args.keys()):
        raise ValueError("DataLoader arguments: \n{0}\n don't match " +
                         "the specification in the dataloader.yaml file:\n{1}".
                         format(set(getargs(CustomDataLoader)), set(dl.args.keys())))

    # Inherit the attributes from dl
    # TODO - make this more automatic / DRY
    # write a method to load those things?
    CustomDataLoader.type = dl.type
    CustomDataLoader.defined_as = dl.defined_as
    CustomDataLoader.args = dl.args
    CustomDataLoader.info = dl.info
    CustomDataLoader.output_schema = dl.output_schema
    CustomDataLoader.dependencies = dl.dependencies
    # keep it hidden?
    CustomDataLoader._yaml_path = yaml_path
    CustomDataLoader.source = source
    # TODO - rename?
    CustomDataLoader.source_dir = dataloader_dir
    return CustomDataLoader


AVAILABLE_DATALOADERS = {"PreloadedDataset": PreloadedDataset,
                         "Dataset": Dataset,
                         "BatchDataset": BatchDataset,
                         "SampleIterator": SampleIterator,
                         "SampleGenerator": SampleGenerator,
                         "BatchIterator": BatchIterator,
                         "BatchGenerator": BatchGenerator}
