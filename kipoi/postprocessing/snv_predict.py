import copy
import copy
import itertools
import os
import tempfile

import cyvcf2
import numpy as np
import pandas as pd
from tqdm import tqdm
import six
import logging

logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())

from kipoi.postprocessing.utils.generic import select_from_model_inputs, OutputReshaper, default_vcf_id_gen, ModelInfoExtractor
from kipoi.postprocessing.utils.io import BedWriter
from kipoi.postprocessing.variant_effects import _process_sequence_set, analyse_model_preds, Logit


def _overlap_vcf_region(vcf_obj, regions, exclude_indels = True):
    """
    Overlap a vcf with regions generated by the dataloader
    The region definition is assumed to be 0-based hence it is converted to 1-based for tabix overlaps!
    Returns VCF records
    """
    assert isinstance(regions["chr"], list) or isinstance(regions["chr"], np.ndarray)
    contained_regions = []
    vcf_records = []
    for i in range(len(regions["chr"])):
        chrom, start, end = regions["chr"][i], regions["start"][i] + 1, regions["end"][i]
        region_str = "{0}:{1}-{2}".format(chrom, start, end)
        variants = vcf_obj(region_str)
        for record in variants:
            if record.is_indel and exclude_indels:
                continue
            vcf_records.append(record)
            contained_regions.append(i)
    #
    return vcf_records, contained_regions



def _generate_seq_sets(relv_seq_keys, dataloader, model_input, vcf_fh, vcf_id_generator_fn, vcf_search_regions = False,
                       array_trafo=None, generate_rc = True):
    """
    Perform in-silico mutagenesis on what the dataloader has returned.  
    
    This function has to convert the DNA regions in the model input according to ref, alt, fwd, rc and
    return a dictionary of which the keys are compliant with evaluation_function arguments
    
    DataLoaders that implement fwd and rc sequence output *__at once__* are not treated in any special way.
    """
    # first establish which VCF region we are talking about...
    ranges_input_objs = {}
    null_key = None
    #print(1)
    # every sequence key can have it's own region definition
    for seq_key in relv_seq_keys:
        #print(seq_key)
        if isinstance(dataloader.output_schema.inputs, dict):
            ranges_slots = dataloader.output_schema.inputs[seq_key].associated_metadata
        elif isinstance(dataloader.output_schema.inputs, list):
            ranges_slots = [x.associated_metadata for x in dataloader.output_schema.inputs if x.name == seq_key][0]
        else:
            ranges_slots = dataloader.output_schema.inputs.associated_metadata
        # check the ranges slots
        if len(ranges_slots) != 1:
            raise ValueError("Exactly one metadata ranges field must defined for a sequence that has to be used for effect precition.")
        #
        # there will at max be one element in the ranges_slots object
        # extract the metadata output
        ranges_input_objs[seq_key] = model_input['metadata'][ranges_slots[0]]
        if null_key is None:
            null_key = seq_key
        else:
            for k in  ["chr", "start", "end", "id"]:
                assert(ranges_input_objs[seq_key][k] == ranges_input_objs[null_key][k])

    # now get the right region from the vcf:
    vcf_records = []
    process_ids = []
    process_lines = []
    #print(2)
    if vcf_search_regions:
        vcf_records, process_lines = _overlap_vcf_region(vcf_fh, ranges_input_objs[null_key])
        process_ids = [ranges_input_objs[null_key]["id"][i] for i in process_lines]
    else:
        for i, returned_id in enumerate(ranges_input_objs[null_key]["id"]):
            for record in vcf_fh:
                id = vcf_id_generator_fn(record)
                if str(id) == str(returned_id):
                    vcf_records.append(record)
                    process_ids.append(returned_id)
                    process_lines.append(i)
                    break
                else:
                    # Warn here...
                    logger.warn("Skipping VCF line (%s) because generated region is for different variant.."%str(id))
                    pass
    #print(3)
    # short-cut if no sequences are left
    if len(process_lines) == 0:
        return None

    # Generate 4 copies of the input set. subset datapoints if needed.
    input_set = {}
    seq_dirs = ["fwd"]
    if generate_rc:
        seq_dirs = ["fwd", "rc"]
    for s_dir, allele in itertools.product(seq_dirs, ["ref", "alt"]):
        k = "%s_%s" % (s_dir, allele)
        #print(k)
        ds = model_input['inputs']
        all_lines = list(range(len(ranges_input_objs[null_key]["id"])))
        if process_ids != process_lines:
            # subset or rearrange elements
            ds = select_from_model_inputs(model_input['inputs'], process_lines, len(all_lines))
        input_set[k] = copy.deepcopy(ds)
    #print(4)

    # Start from the sequence inputs mentioned in the model.yaml
    for seq_key in relv_seq_keys:
        #print(str(seq_key) + str(1))
        # extract the metadata output
        ranges_input_obj = ranges_input_objs[seq_key]
        #
        # Assemble variant modification information
        preproc_conv = {"pp_line":[], "varpos_rel":[], "ref":[], "alt":[], "start":[], "end":[], "id":[]}

        if ("strand" in ranges_input_obj) and (isinstance(ranges_input_obj["strand"], list) or
                                                   isinstance(ranges_input_obj["strand"], np.ndarray)):
            preproc_conv["strand"] = []

        for i, record in enumerate(vcf_records):
            ranges_input_i = process_lines[i]
            assert process_ids[i] == ranges_input_obj["id"][ranges_input_i]
            assert not record.is_indel # Catch indels, that needs a slightly modified processing
            preproc_conv["start"].append(ranges_input_obj["start"][ranges_input_i]+1) # convert bed back to 1-based
            preproc_conv["end"].append(ranges_input_obj["end"][ranges_input_i])
            preproc_conv["varpos_rel"].append(int(record.POS) - preproc_conv["start"][-1])
            if (preproc_conv["varpos_rel"][-1] < 0) or\
                    (preproc_conv["varpos_rel"][-1] > (preproc_conv["end"][-1] - preproc_conv["start"][-1]+1)):
                raise Exception("Variant does not lie in suggested region!")

            preproc_conv["ref"].append(str(record.REF))
            preproc_conv["alt"].append(str(record.ALT[0]))
            preproc_conv["id"].append(str(process_ids[i]))
            preproc_conv["pp_line"].append(i)
            if "strand" in preproc_conv:
                preproc_conv["strand"].append(ranges_input_obj["strand"][ranges_input_i])


        # If strand wasn't a list then try to still fix it..
        if "strand" not in preproc_conv:
            if "strand" not in ranges_input_obj:
                preproc_conv["strand"] = ["*"] * len(ranges_input_obj["chr"])
            elif isinstance(ranges_input_obj["strand"], six.string_types):
                preproc_conv["strand"] = [ranges_input_obj["strand"]] * len(ranges_input_obj["chr"])
            else:
                raise Exception("Strand defintion invalid in metadata returned by dataloader.")

        preproc_conv_df = pd.DataFrame(preproc_conv)

        #print(str(seq_key) + str(3))
        # Actually modify sequences according to annotation
        for s_dir, allele in itertools.product(seq_dirs, ["ref", "alt"]):
            k = "%s_%s" % (s_dir, allele)
            #print(str(seq_key) + str(k))
            if isinstance(dataloader.output_schema.inputs, dict):
                if seq_key not in input_set[k]:
                    raise Exception("Sequence field %s is missing in DataLoader output!" % seq_key)
                input_set[k][seq_key] = _process_sequence_set(input_set[k][seq_key], preproc_conv_df, allele, s_dir, array_trafo)
            elif isinstance(dataloader.output_schema.inputs, list):
                modified_set = []
                for seq_el, input_schema_el in zip(input_set[k], dataloader.output_schema.inputs):
                    if input_schema_el.name == seq_key:
                        modified_set.append(_process_sequence_set(seq_el, preproc_conv_df, allele, s_dir, array_trafo))
                    else:
                        modified_set.append(seq_el)
                input_set[k] = modified_set
            else:
                input_set[k] = _process_sequence_set(input_set[k], preproc_conv_df, allele, s_dir, array_trafo)

    #print(5)
    #
    # Reformat so that effect prediction function will get its required inputs
    pred_set = {"ref": input_set["fwd_ref"], "alt": input_set["fwd_alt"]}
    if generate_rc:
        pred_set["ref_rc"] = input_set["rc_ref"]
        pred_set["alt_rc"] = input_set["rc_alt"]
    pred_set["mutation_positions"] = preproc_conv_df["varpos_rel"].values
    pred_set["line_id"] = preproc_conv_df["id"].values
    pred_set["vcf_records"] = vcf_records
    return pred_set


def predict_snvs(model,
                 dataloader,
                 vcf_fpath,
                 batch_size,
                 num_workers=0,
                 dataloader_args=None,
                 vcf_to_region=None,
                 vcf_id_generator_fn=default_vcf_id_gen,
                 evaluation_function=analyse_model_preds,
                 evaluation_function_kwargs={'diff_types': {'logit': Logit()}},
                 sync_pred_writer=None,
                 use_dataloader_example_data=False,
                 return_predictions=False,
                 generated_seq_writer=None
                 ):
    """Predict the effect of SNVs

            Prediction of effects of SNV based on a VCF. If desired the VCF can be stored with the predicted values as
            annotation. For a detailed description of the requirements in the yaml files please take a look at
            kipoi/nbs/variant_effect_prediction.ipynb.

            # Arguments
                model: A kipoi model handle generated by e.g.: kipoi.get_model()
                dataloader: Dataloader factory generated by e.g.: kipoi.get_dataloader_factory()
                vcf_fpath: Path of the VCF defining the positions that shall be assessed. Only SNVs will be tested.
                batch_size: Prediction batch size used for calling the data loader. Each batch will be generated in 4
                    mutated states yielding a system RAM consumption of >= 4x batch size.
                num_workers: Number of parallel workers for loading the dataset.
                dataloader_args: arguments passed on to the dataloader for sequence generation, arguments
                    mentioned in dataloader.yaml > postprocessing > variant_effects > bed_input will be overwritten
                    by the methods here.
                vcf_to_region: Callable that generates a region compatible with dataloader/model from a cyvcf2 record
                vcf_id_generator_fn: Callable that generates a unique ID from a cyvcf2 record
                evaluation_function: effect evaluation function. Default is `analyse_model_preds`, which will get
                    arguments defined in `evaluation_function_kwargs`
                evaluation_function_kwargs: kwargs passed on to `evaluation_function`.
                sync_pred_writer: Single writer or list of writer objects like instances of `VcfWriter`. This object
                    will be called after effect prediction of a batch is done.
                use_dataloader_example_data: Fill out the missing dataloader arguments with the example values given in the
                    dataloader.yaml.
                return_predictions: Return all variant effect predictions as a dictionary. Setting this to False will
                    help maintain a low memory profile and is faster as it avoids concatenating batches after prediction.
                generated_seq_writer: Single writer or list of writer objects like instances of `SyncHdf5SeqWriter`.
                    This object will be called after the DNA sequence sets have been generated. If this parameter is
                    not None, no prediction will be performed and only DNA sequence will be written!! This is relevant
                    if you want to use the `predict_snvs` to generate appropriate input DNA sequences for your model.

            # Returns
                If return_predictions: Dictionary which contains a pandas DataFrame containing the calculated values
                    for each model output (target) column VCF SNV line. If return_predictions == False, returns None.
            """
    model_info_extractor = ModelInfoExtractor(model_obj=model, dataloader_obj=dataloader)
    seq_fields = model_info_extractor.seq_fields

    dna_seq_trafo = model_info_extractor.dna_seq_trafo

    # If then where do I have to put my bed file in the command?

    exec_files_bed_keys = model_info_extractor.get_exec_files_bed_keys()
    temp_bed3_file = None

    vcf_search_regions = True

    # If there is a field for putting the a postprocessing bed file, then generate the bed file.
    if exec_files_bed_keys is not None:
        vcf_search_regions = False
        if vcf_to_region is None:
            raise Exception("vcf_to_region parameter has to be set if regions should be generated from a VCF."
                            "(Requested by defining: postprocessing > variant_effects > bed_input in: dataloader.yaml)")

        temp_bed3_file = tempfile.mktemp()  # file path of the temp file

        vcf_fh = cyvcf2.VCF(vcf_fpath, "r")

        with BedWriter(temp_bed3_file) as ofh:
            for record in vcf_fh:
                if not record.is_indel:
                    region = vcf_to_region(record)
                    id = vcf_id_generator_fn(record)
                    for chrom, start, end in zip(region["chrom"], region["start"], region["end"]):
                        ofh.append_interval(chrom=chrom, start=start, end=end, id=id)

        vcf_fh.close()

    # Assemble the paths for executing the dataloader
    if dataloader_args is None:
        dataloader_args = {}

    # Copy the missing arguments from the example arguments.
    if use_dataloader_example_data:
        for k in dataloader.example_kwargs:
            if k not in dataloader_args:
                dataloader_args[k] = dataloader.example_kwargs[k]

    # If there was a field for dumping the region definition bed file, then use it.
    if exec_files_bed_keys is not None:
        for k in exec_files_bed_keys:
            dataloader_args[k] = temp_bed3_file

    model_out_annotation = model_info_extractor.get_model_out_annotation()

    out_reshaper = OutputReshaper(model.schema.targets)

    res = []

    it = dataloader(**dataloader_args).batch_iter(batch_size=batch_size,
                                                  num_workers=num_workers)

    # organise the writers in a list
    if sync_pred_writer is not None:
        if not isinstance(sync_pred_writer, list):
            sync_pred_writer = [sync_pred_writer]

    # organise the prediction writers
    if generated_seq_writer is not None:
        if not isinstance(generated_seq_writer, list):
            generated_seq_writer = [generated_seq_writer]

    # Open vcf again
    vcf_fh = cyvcf2.VCF(vcf_fpath, "r")

    # pre-process regions
    keys = set()

    for i, batch in enumerate(tqdm(it)):
        # For debugging
        # if i >= 10:
        #     break
        # becomes noticable for large vcf's. Is there a way to avoid it? (i.e. to exploit the iterative nature of dataloading)
        eval_kwargs = _generate_seq_sets(seq_fields, dataloader, batch, vcf_fh, default_vcf_id_gen,
                                         vcf_search_regions = vcf_search_regions,
                                         array_trafo=dna_seq_trafo, generate_rc=model_info_extractor.supports_simple_rc)
        if eval_kwargs is None:
            # No generated datapoint overlapped any VCF region
            continue

        if generated_seq_writer is not None:
            for writer in generated_seq_writer:
                writer(eval_kwargs)
            # Assume that we don't actually want the predictions to be calculated...
            continue

        if evaluation_function_kwargs is not None:
            assert isinstance(evaluation_function_kwargs, dict)
            for k in evaluation_function_kwargs:
                eval_kwargs[k] = evaluation_function_kwargs[k]

        eval_kwargs["out_annotation_all_outputs"] = model_out_annotation

        res_here = evaluation_function(model, output_reshaper=out_reshaper, **eval_kwargs)
        for k in res_here:
            keys.add(k)
            res_here[k].index = eval_kwargs["line_id"]
        # write the predictions synchronously
        if sync_pred_writer is not None:
            for writer in sync_pred_writer:
                writer(res_here, eval_kwargs["vcf_records"])
        if return_predictions:
            res.append(res_here)

    vcf_fh.close()

    try:
        if temp_bed3_file is not None:
            os.unlink(temp_bed3_file)
    except:
        pass

    if return_predictions:
        res_concatenated = {}
        for k in keys:
            res_concatenated[k] = pd.concat([batch[k]
                                             for batch in res
                                             if k in batch])
        return res_concatenated

    return None

