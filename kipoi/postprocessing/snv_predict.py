import copy
import copy
import itertools
import os
import tempfile

import cyvcf2
import numpy as np
import pandas as pd
from tqdm import tqdm

from kipoi.postprocessing.utils.generic import select_from_model_inputs, Output_reshaper, _default_vcf_id_gen
from kipoi.postprocessing.utils.io import Bed_writer
from kipoi.postprocessing.variant_effects import _process_sequence_set, analyse_model_preds, Logit


def _overlap_vcf_region(vcf_obj, regions, exclude_indels = True):
    """
    Overlap a vcf with regions generated by the dataloader
    The region definition is assumed to be 0-based hence it is converted to 1-based for tabix overlaps!
    Returns VCF records
    """
    assert isinstance(regions["chr"], list) or isinstance(regions["chr"], np.ndarray)
    contained_regions = []
    vcf_records = []
    for i in range(len(regions["chr"])):
        chrom, start, end = regions["chr"][i], regions["start"][i] + 1, regions["end"][i]
        region_str = "{0}:{1}-{2}".format(chrom, start, end)
        variants = vcf_obj(region_str)
        for record in variants:
            if record.is_indel and exclude_indels:
                continue
            vcf_records.append(record)
            contained_regions.append(i)
    #
    return vcf_records, contained_regions



def _generate_seq_sets(relv_seq_keys, dataloader, model_input, vcf_fh, vcf_id_generator_fn, vcf_search_regions = False,
                       array_trafo=None, generate_rc = True):
    """
    Perform in-silico mutagenesis on what the dataloader has returned.  
    
    This function has to convert the DNA regions in the model input according to ref, alt, fwd, rc and
    return a dictionary of which the keys are compliant with evaluation_function arguments
    
    DataLoaders that implement fwd and rc sequence output *__at once__* are not treated in any special way.
    """
    # first establish which VCF region we are talking about...
    ranges_input_objs = {}
    null_key = None
    # every sequence key can have it's own region definition
    for seq_key in relv_seq_keys:
        if isinstance(dataloader.output_schema.inputs, dict):
            ranges_slots = dataloader.output_schema.inputs[seq_key].associated_metadata
        elif isinstance(dataloader.output_schema.inputs, list):
            ranges_slots = [x.associated_metadata for x in dataloader.output_schema.inputs if x.name == seq_key][0]
        else:
            ranges_slots = dataloader.output_schema.inputs.associated_metadata
        # check the ranges slots
        if len(ranges_slots) != 1:
            raise ValueError("Exactly one metadata ranges field must defined for a sequence that has to be used for effect precition.")
        #
        # there will at max be one element in the ranges_slots object
        # extract the metadata output
        ranges_input_objs[seq_key] = model_input['metadata'][ranges_slots[0]]
        if null_key is None:
            null_key = seq_key
        else:
            for k in  ["chr", "start", "end", "id"]:
                assert(ranges_input_objs[seq_key][k] == ranges_input_objs[null_key][k])

    # now get the right region from the vcf:
    vcf_records = []
    process_ids = []
    process_lines = []
    if vcf_search_regions:
        vcf_records, process_lines = _overlap_vcf_region(vcf_fh, ranges_input_objs[null_key])
        process_ids = [ranges_input_objs[null_key]["id"][i] for i in process_lines]
    else:
        for i, returned_id in enumerate(ranges_input_objs[null_key]["id"]):
            for record in vcf_fh:
                id = vcf_id_generator_fn(record)
                if str(id) == str(returned_id):
                    vcf_records.append(record)
                    process_ids.append(returned_id)
                    process_lines.append(i)
                    break
                else:
                    # Warn here...
                    pass

    # short-cut if no sequences are left
    if len(process_lines) == 0:
        return None

    # Generate 4 copies of the input set. subset datapoints if needed.
    input_set = {}
    seq_dirs = ["fwd"]
    if generate_rc:
        seq_dirs = ["fwd", "rc"]
    for s_dir, allele in itertools.product(seq_dirs, ["ref", "alt"]):
        k = "%s_%s" % (s_dir, allele)
        ds = model_input['inputs']
        all_lines = list(range(len(ranges_input_objs[null_key]["id"])))
        if process_ids != process_lines:
            # subset or rearrange elements
            ds = select_from_model_inputs(model_input['inputs'], process_lines, len(all_lines))
        input_set[k] = copy.deepcopy(ds)


    # Start from the sequence inputs mentioned in the model.yaml
    for seq_key in relv_seq_keys:
        # extract the metadata output
        ranges_input_obj = ranges_input_objs[seq_key]
        #
        # Assemble variant modification information
        preproc_conv = {"pp_line":[], "varpos_rel":[], "strand":[], "ref":[], "alt":[], "start":[], "end":[], "id":[]}

        for i, record in enumerate(vcf_records):
            assert process_ids[i] == ranges_input_obj["id"][i]
            assert not record.is_indel # Catch indels, that needs a slightly modified processing
            preproc_conv["start"].append(ranges_input_obj["start"][i]+1) # convert bed back to 1-based
            preproc_conv["end"].append(ranges_input_obj["end"][i])
            preproc_conv["varpos_rel"].append(int(record.POS) - preproc_conv["start"][-1])
            if (preproc_conv["varpos_rel"][-1] < 0) or\
                    (preproc_conv["varpos_rel"][-1] > (preproc_conv["end"][-1] - preproc_conv["start"][-1]+1)):
                raise Exception("Variant does not lie in suggested region!")

            preproc_conv["ref"].append(str(record.REF))
            preproc_conv["alt"].append(str(record.ALT[0]))
            preproc_conv["id"].append(str(process_ids[i]))
            preproc_conv["pp_line"].append(i)


        if "strand" in ranges_input_obj:
            preproc_conv["strand"] = ranges_input_obj["strand"]
        else:
            preproc_conv["strand"] = ["*"] * len(ranges_input_obj["chr"])

        preproc_conv_df = pd.DataFrame(preproc_conv)

        if preproc_conv_df.shape[0] != len(ranges_input_obj["id"]):
            raise Exception("Error, id mismatch between generated sequences and VCF.")


        # Actually modify sequences according to annotation
        for s_dir, allele in itertools.product(seq_dirs, ["ref", "alt"]):
            k = "%s_%s" % (s_dir, allele)
            if isinstance(dataloader.output_schema.inputs, dict):
                if seq_key not in input_set[k]:
                    raise Exception("Sequence field %s is missing in DataLoader output!" % seq_key)
                input_set[k][seq_key] = _process_sequence_set(input_set[k][seq_key], preproc_conv_df, allele, s_dir, array_trafo)
            elif isinstance(dataloader.output_schema.inputs, list):
                modified_set = []
                for seq_el, input_schema_el in zip(input_set[k], dataloader.output_schema.inputs):
                    if input_schema_el.name == seq_key:
                        modified_set.append(_process_sequence_set(seq_el, preproc_conv_df, allele, s_dir, array_trafo))
                    else:
                        modified_set.append(seq_el)
                input_set[k] = modified_set
            else:
                input_set[k] = _process_sequence_set(input_set[k], preproc_conv_df, allele, s_dir, array_trafo)
    #
    # Reformat so that effect prediction function will get its required inputs
    pred_set = {"ref": input_set["fwd_ref"], "alt": input_set["fwd_alt"]}
    if generate_rc:
        pred_set["ref_rc"] = input_set["rc_ref"]
        pred_set["alt_rc"] = input_set["rc_alt"]
    pred_set["mutation_positions"] = preproc_conv_df["varpos_rel"].values
    pred_set["line_id"] = preproc_conv_df["id"].values
    pred_set["vcf_records"] = vcf_records
    return pred_set


def predict_snvs(model_info_extractor,
                 vcf_fpath,
                 batch_size,
                 num_workers=0,
                 dataloader_args=None,
                 vcf_to_region=None,
                 vcf_id_generator_fn=_default_vcf_id_gen,
                 evaluation_function=analyse_model_preds,
                 evaluation_function_kwargs={'diff_types': {'logit': Logit()}},
                 sync_pred_writer=None,
                 use_dataloader_example_data=False,
                 return_predictions=False
                 ):
    """Predict the effect of SNVs

            Prediction of effects of SNV based on a VCF. If desired the VCF can be stored with the predicted values as
            annotation. For a detailed description of the requirements in the yaml files please take a look at
            kipoi/nbs/variant_effect_prediction.ipynb.

            # Arguments
                model: A kipoi model handle generated by e.g.: kipoi.get_model()
                vcf_fpath: Path of the VCF defining the positions that shall be assessed. Only SNVs will be tested.
                dataloader: Dataloader factory generated by e.g.: kipoi.get_dataloader_factory()
                batch_size: Prediction batch size used for calling the data loader. Each batch will be generated in 4
                    mutated states yielding a system RAM consumption of >= 4x batch size.
                num_workers: Number of parallel workers for loading the dataset.
                dataloader_arguments: arguments passed on to the dataloader for sequence generation, arguments
                    mentioned in dataloader.yaml > postprocessing > variant_effects > bed_input will be overwritten
                    by the methods here.
                model_out_annotation: Columns of the model output can be passed here, otherwise they will be attepted to be
                    loaded from model.yaml > schema > targets > column_labels
                evaluation_function: effect evaluation function. Default is ism
                evaluation_function_kwargs: kwargs passed on to the evaluation function.
                out_vcf_fpath: Path for the annotated VCF, which is created from `vcf_fpath` one predicted effect is
                    produced for every output (target) column.
                use_dataloader_example_data: Fill out the missing dataloader arguments with the example values given in the
                    dataloader.yaml.

            # Returns
                Dictionary which contains a pandas DataFrame containing the calculated values
                    for each model output (target) column VCF SNV line
            """
    seq_fields = model_info_extractor.seq_fields

    model = model_info_extractor.model
    dataloader = model_info_extractor.dataloader

    dna_seq_trafo = model_info_extractor.dna_seq_trafo

    # If then where do I have to put my bed file in the command?

    exec_files_bed_keys = model_info_extractor.get_exec_files_bed_keys()
    temp_bed3_file = None


    # If there is a field for putting the a postprocessing bed file, then generate the bed file.
    if exec_files_bed_keys is not None:

        if vcf_to_region is None:
            raise Exception("vcf_to_region parameter has to be set if regions are generated from a VCF.")

        temp_bed3_file = tempfile.mktemp()  # file path of the temp file

        vcf_fh = cyvcf2.VCF(vcf_fpath, "r")

        with Bed_writer(temp_bed3_file) as ofh:
            for record in vcf_fh:
                if not record.is_indel:
                    region = vcf_to_region(record)
                    id = vcf_id_generator_fn(record)
                    for chrom, start, end in zip(region["chrom"], region["start"], region["end"]):
                        ofh.append_interval(chrom=chrom, start=start, end=end, id=id)

        vcf_fh.close()

    # Assemble the paths for executing the dataloader
    if dataloader_args is None:
        dataloader_args = {}

    # Copy the missing arguments from the example arguments.
    if use_dataloader_example_data:
        for k in dataloader.example_kwargs:
            if k not in dataloader_args:
                dataloader_args[k] = dataloader.example_kwargs[k]

    # If there was a field for dumping the region definition bed file, then use it.
    if exec_files_bed_keys is not None:
        for k in exec_files_bed_keys:
            dataloader_args[k] = temp_bed3_file

    model_out_annotation = model_info_extractor.get_model_out_annotation()

    out_reshaper = Output_reshaper(model.schema.targets)

    res = []

    it = dataloader(**dataloader_args).batch_iter(batch_size=batch_size,
                                                  num_workers=num_workers)

    # organise the writers in a list
    if sync_pred_writer is not None:
        if not isinstance(sync_pred_writer, list):
            sync_pred_writer = [sync_pred_writer]

    # Open vcf again
    vcf_fh = cyvcf2.VCF(vcf_fpath, "r")

    # pre-process regions
    keys = set()
    import pdb
    #pdb.set_trace()
    for i, batch in enumerate(tqdm(it)):
        # For debugging
        # if i >= 10:
        #     break
        # becomes noticable for large vcf's. Is there a way to avoid it? (i.e. to exploit the iterative nature of dataloading)
        eval_kwargs = _generate_seq_sets(seq_fields, dataloader, batch, vcf_fh, _default_vcf_id_gen,
                                         array_trafo=dna_seq_trafo, generate_rc=model_info_extractor.supports_simple_rc)
        if eval_kwargs is None:
            # No generated datapoint overlapped any VCF region
            continue

        if evaluation_function_kwargs is not None:
            assert isinstance(evaluation_function_kwargs, dict)
            for k in evaluation_function_kwargs:
                eval_kwargs[k] = evaluation_function_kwargs[k]

        eval_kwargs["out_annotation_all_outputs"] = model_out_annotation

        res_here = evaluation_function(model, output_reshaper=out_reshaper, **eval_kwargs)
        for k in res_here:
            keys.add(k)
            res_here[k].index = eval_kwargs["line_id"]
        # write the predictions synchronously
        import pdb
        #pdb.set_trace()
        if sync_pred_writer is not None:
            for writer in sync_pred_writer:
                writer(res_here, eval_kwargs["vcf_records"])
        if return_predictions:
            res.append(res_here)

    vcf_fh.close()

    try:
        if temp_bed3_file is not None:
            os.unlink(temp_bed3_file)
    except:
        pass

    if return_predictions:
        res_concatenated = {}
        for k in keys:
            res_concatenated[k] = pd.concat([batch[k]
                                             for batch in res
                                             if k in batch])
        return res_concatenated

    return None

