{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing models\n",
    "\n",
    "by Ziga Avsec\n",
    "\n",
    "Composing models means that we take the predictions of some model and use it as input for another model like this:\n",
    "\n",
    "![img](../docs/theme_dir/img/nbs/comp_models.gv.svg)\n",
    "\n",
    "Three different scenarios can occur when we want to compose models from Kipoi:\n",
    "\n",
    "1. all models are written in the same framework (say Keras)\n",
    "2. models are written in different frameworks but can all be executed in the same python environment\n",
    "3. models are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All models in the same framework\n",
    "\n",
    "In case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras.\n",
    "\n",
    "![img](../docs/theme_dir/img/nbs/comp_models3.gv.svg)\n",
    "\n",
    "Let's first dump 4 dummy models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as kl\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model 1\n",
    "inp1 = kl.Input((3,), name=\"input1\")\n",
    "out1 = kl.Dense(4)(inp1)\n",
    "m1 = Model(inp1, out1)\n",
    "m1.save(\"/tmp/m1.h5\")\n",
    "\n",
    "# create model 2\n",
    "inp2 = kl.Input((7,), name=\"input1_model1\")\n",
    "out2 = kl.Dense(3)(inp2)\n",
    "m2 = Model(inp2, out2)\n",
    "m2.save(\"/tmp/m2.h5\")\n",
    "\n",
    "# create model 3\n",
    "inp3 = kl.Input((6,), name=\"input2\")\n",
    "out3 = kl.Dense(4)(inp3)\n",
    "m3 = Model(inp3, out3)\n",
    "m3.save(\"/tmp/m3.h5\")\n",
    "\n",
    "# create model 4\n",
    "inp4 = kl.Input((7,), name=\"model2_model3\")\n",
    "out4 = kl.Dense(1)(inp4)\n",
    "m4 = Model(inp4, out4)\n",
    "m4.save(\"/tmp/m4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the models back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "## Load models\n",
    "m1 = load_model(\"/tmp/m1.h5\")\n",
    "m2 = load_model(\"/tmp/m2.h5\")\n",
    "m3 = load_model(\"/tmp/m3.h5\")\n",
    "m4 = load_model(\"/tmp/m4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compose them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_in = kl.concatenate([m1.output, m1.input])\n",
    "m2_out = m2(m2_in)\n",
    "\n",
    "m3_in = kl.concatenate([m2_out, m3.output])\n",
    "out = m4(m3_in)\n",
    "\n",
    "m = Model(inputs=[m1.input, m3.input], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 335.31 410.00\" width=\"335pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-406 331.305,-406 331.305,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139943309246648 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139943309246648</title>\n",
       "<polygon fill=\"none\" points=\"36.9397,-365.5 36.9397,-401.5 158.305,-401.5 158.305,-365.5 36.9397,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.6224\" y=\"-379.3\">input1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139943309246536 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139943309246536</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 111.245,-328.5 111.245,-292.5 0,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"55.6224\" y=\"-306.3\">dense_25: Dense</text>\n",
       "</g>\n",
       "<!-- 139943309246648&#45;&gt;139943309246536 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139943309246648-&gt;139943309246536</title>\n",
       "<path d=\"M87.4554,-365.313C82.5028,-356.941 76.4432,-346.697 70.9451,-337.403\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"73.7996,-335.354 65.6958,-328.529 67.7748,-338.918 73.7996,-335.354\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139943309863400 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139943309863400</title>\n",
       "<polygon fill=\"none\" points=\"7.7973,-219.5 7.7973,-255.5 187.447,-255.5 187.447,-219.5 7.7973,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.6224\" y=\"-233.3\">concatenate_20: Concatenate</text>\n",
       "</g>\n",
       "<!-- 139943309246648&#45;&gt;139943309863400 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139943309246648-&gt;139943309863400</title>\n",
       "<path d=\"M107.166,-365.256C112.263,-355.05 118.013,-341.675 120.622,-329 123.938,-312.893 123.938,-308.107 120.622,-292 118.747,-282.89 115.249,-273.418 111.537,-265.021\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"114.594,-263.298 107.166,-255.744 108.262,-266.282 114.594,-263.298\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139943309246536&#45;&gt;139943309863400 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139943309246536-&gt;139943309863400</title>\n",
       "<path d=\"M65.7894,-292.313C70.742,-283.941 76.8016,-273.697 82.2997,-264.403\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.47,-265.918 87.549,-255.529 79.4452,-262.354 85.47,-265.918\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139944819902392 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139944819902392</title>\n",
       "<polygon fill=\"none\" points=\"57.6543,-146.5 57.6543,-182.5 173.59,-182.5 173.59,-146.5 57.6543,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115.622\" y=\"-160.3\">model_33: Model</text>\n",
       "</g>\n",
       "<!-- 139943309863400&#45;&gt;139944819902392 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139943309863400-&gt;139944819902392</title>\n",
       "<path d=\"M101.98,-219.313C104.036,-211.202 106.537,-201.336 108.834,-192.277\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"112.24,-193.082 111.305,-182.529 105.455,-191.362 112.24,-193.082\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139943311642976 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139943311642976</title>\n",
       "<polygon fill=\"none\" points=\"205.94,-219.5 205.94,-255.5 327.305,-255.5 327.305,-219.5 205.94,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.622\" y=\"-233.3\">input2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139943309603560 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139943309603560</title>\n",
       "<polygon fill=\"none\" points=\"201,-146.5 201,-182.5 312.245,-182.5 312.245,-146.5 201,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.622\" y=\"-160.3\">dense_27: Dense</text>\n",
       "</g>\n",
       "<!-- 139943311642976&#45;&gt;139943309603560 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139943311642976-&gt;139943309603560</title>\n",
       "<path d=\"M264.202,-219.313C263.072,-211.289 261.7,-201.547 260.435,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"263.881,-191.943 259.021,-182.529 256.95,-192.919 263.881,-191.943\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139943312609520 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139943312609520</title>\n",
       "<polygon fill=\"none\" points=\"95.7973,-73.5 95.7973,-109.5 275.447,-109.5 275.447,-73.5 95.7973,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185.622\" y=\"-87.3\">concatenate_21: Concatenate</text>\n",
       "</g>\n",
       "<!-- 139944819902392&#45;&gt;139943312609520 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139944819902392-&gt;139943312609520</title>\n",
       "<path d=\"M132.567,-146.313C141.338,-137.417 152.191,-126.409 161.791,-116.672\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"164.305,-119.107 168.833,-109.529 159.32,-114.193 164.305,-119.107\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139943309603560&#45;&gt;139943312609520 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139943309603560-&gt;139943312609520</title>\n",
       "<path d=\"M239.435,-146.313C230.54,-137.417 219.532,-126.409 209.794,-116.672\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"212.197,-114.125 202.651,-109.529 207.247,-119.075 212.197,-114.125\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139943309585488 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139943309585488</title>\n",
       "<polygon fill=\"none\" points=\"127.654,-0.5 127.654,-36.5 243.59,-36.5 243.59,-0.5 127.654,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185.622\" y=\"-14.3\">model_35: Model</text>\n",
       "</g>\n",
       "<!-- 139943312609520&#45;&gt;139943309585488 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139943312609520-&gt;139943309585488</title>\n",
       "<path d=\"M185.622,-73.3129C185.622,-65.2895 185.622,-55.5475 185.622,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.122,-46.5288 185.622,-36.5288 182.122,-46.5289 189.122,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "svg_img = model_to_dot(m, ).create(prog='dot', format='svg')\n",
    "SVG(svg_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting `m1.trainable = False`.\n",
    "\n",
    "### Contributing to Kipoi\n",
    "\n",
    "To contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models in different frameworks\n",
    "\n",
    "There are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses `keras=0.3` and and another one `keras=2.0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatible dependencies\n",
    "\n",
    "To compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model.\n",
    "\n",
    "![img](../docs/theme_dir/img/nbs/comp_models2.gv.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1):\n",
    "    m1 = kipoi.get_model(\"model1\")\n",
    "    m2 = kipoi.get_model(\"model2\")\n",
    "    m3 = kipoi.get_model(\"model3\")\n",
    "    \n",
    "    dl1 = m1.default_dataloader(**dl1_kwargs)\n",
    "    dl2 = m1.default_dataloader(**dl2_kwargs)\n",
    "    \n",
    "    target_gen = get_target_gen(target_file)\n",
    "    \n",
    "    batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers)\n",
    "    batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    while True:\n",
    "        batch1 = next(batch_it1)['inputs']\n",
    "        batch2 = next(batch_it2)['inputs']\n",
    "        targets, ids = next(target_gen)\n",
    "        \n",
    "        m1_pred = m1.predict_on_batch(batch1)\n",
    "        m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1))\n",
    "        m3_pred = m3.predict_on_batch(batch2)\n",
    "        yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \n",
    "               \"targets\": targets, \n",
    "               \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"],\n",
    "                            \"model3_id\": batch2[\"metadata\"][\"id\"],\n",
    "                            \"targets_id\": ids,\n",
    "                           }\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model 4\n",
    "inp2 = kl.Input((3,), name=\"model2\")\n",
    "inp3 = kl.Input((4,), name=\"model3\")\n",
    "x = kl.concatenate([inp2, inp3])\n",
    "out4 = kl.Dense(1)(x)\n",
    "m4 = Model([inp2, inp3], out4)\n",
    "m4.compile('rmsprop',\n",
    "           loss='categorical_crossentropy',\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model4\n",
    "def create_train_gen(**kwargs):\n",
    "    while True:\n",
    "        gen = new_dataloader(**kwargs)\n",
    "        while True:\n",
    "            batch = next(gen)\n",
    "            yield (batch['inputs'], batch['targets'])\n",
    "            \n",
    "train_gen = create_train_gen(...)            \n",
    "m4.fit_generator(train_gen, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump model4\n",
    "m4.save(\"model_files/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incompatible dependencies\n",
    "\n",
    "Sometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies. \n",
    "\n",
    "In that case, we should run the prediction of each model in a separate environment and save the predictions to the disk.\n",
    "\n",
    "Luckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake <http://snakemake.readthedocs.io/>. I'll show you how to do this in snakemake.\n",
    "\n",
    "Let's consider the following case:\n",
    "\n",
    "![img](../docs/theme_dir/img/nbs/comp_models4.gv.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python part of the Snakefile\n",
    "import os\n",
    "import subprocess\n",
    "py_path = subprocess.check_output(['which', 'python']).decode().strip()\n",
    "env_paths = os.path.join(os.path.dirname(py_path), \"../envs\")\n",
    "\n",
    "def get_args(wildcards):\n",
    "    \"\"\"Function returning a dictionary of dataloader kwargs\n",
    "    for the corresponding model\n",
    "    \"\"\"\n",
    "    if wildcards.model == \"model3\":\n",
    "        return {\"arg1\": 1}\n",
    "    elif wildcards.model == \"model3\":\n",
    "        return {\"\"}\n",
    "    else:\n",
    "        return {\"arg2\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# Yaml part of the Snakefile\n",
    "rule all:\n",
    "  inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"])\n",
    "\n",
    "rule create_evironment:\n",
    "  \"\"\"Create a new conda environment for each model\"\"\"\n",
    "  output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\")\n",
    "  shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\"\n",
    "\n",
    "rule run_predictions:\n",
    "  \"\"\"Create a new conda environment for each model\"\"\"\n",
    "  input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\")\n",
    "  output: \"predictions/{model}.h5\"\n",
    "  params:\n",
    "    dl_args: get_args\n",
    "    batch_size: 15\n",
    "  threads: 8\n",
    "  shell: \n",
    "      \"\"\"\n",
    "      source activate kipoi-{wildcards.model}\n",
    "      kipoi predict {wildcards.model} \\\n",
    "        -n {threads} \\\n",
    "        --dataloader_args='{params.dl_args}' \\\n",
    "        --batch_size={params.batch_size} \\\n",
    "        -f hdf5 \\\n",
    "        -o {output} \n",
    "      \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snakefile will generate the following hdf5 files\n",
    "\n",
    "- `predictions/model1.h5`\n",
    "- `predictions/model2.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine them, let's write new dataloader, taking as input the hdf5 files containing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdish\n",
    "\n",
    "\n",
    "def new_dataloader(model1_h5, model2_h5, target_file):\n",
    "    d1 = deepdish.io.load(model1_h5)\n",
    "    d2 = deepdish.io.load(model2_h5)\n",
    "    targets = load_target_file(target_file)\n",
    "    return {\n",
    "        \"inputs\": {\n",
    "            \"model1\": d1[\"predictions\"],\n",
    "            \"model2\": d2[\"predictions\"],\n",
    "        },\n",
    "        \"targets\": targets,\n",
    "        \"metadata\": {\n",
    "            \"model1_id\": d1[\"metdata\"][\"id\"],\n",
    "            \"model2_id\": d2[\"metdata\"][\"id\"],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training data ...\n",
    "data_train = new_dataloader(\"predictions/model1.h5\"\n",
    "                            \"predictions/model1.h5\",\n",
    "                            \"target_file.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model...\n",
    "m4.fit(data_train['inputs'], data_train['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the model\n",
    "m4.save(\"model_files/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading composite models to Kipoi\n",
    "\n",
    "Since every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so:\n",
    "\n",
    "- All models in the same framework\n",
    "    - **Dataloader:** newly written, combines dataloaders\n",
    "    - **Model:** combines models by stitching them together in the framework\n",
    "- Different frameworks, compatible dependencies\n",
    "    - **Dataloader:** newly written, combines dataloaders and models\n",
    "    - **Model:** final ensembling model (model 4)\n",
    "- Different frameworks, in-compatible dependencies\n",
    "    - **Dataloader:** newly written, loads data from the hdf5 files containing model predictions\n",
    "    - **Model:** final ensembling model (model 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
